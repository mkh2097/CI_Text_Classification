{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SKlearn-Final-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPdX4P5+FA85ud4U7mAMKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkh2097/CI_Text_Recognition/blob/main/SKlearn_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM4-ioRayK_c",
        "outputId": "9e1bd845-84fe-4b29-f3d8-6603fc33b779"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.2MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 16.0MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154294 sha256=12fe9912cfb4a6de2f456a0fd205cd0a6f3891a48def0866c7e2b76a69e14474\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394467 sha256=c6b57277e4743f436234053c78040323e7495a11aa699c5f2695473de8502a24\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DfNowO8cGr1",
        "outputId": "13ee5fb1-bee5-4e5d-acaa-d4c1f718bf0c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTDiDROIcgG5",
        "outputId": "d82ab6ac-d59c-4236-b0a9-6eb0d7d41bf5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhhJFqJvcdBX"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/test.csv.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJvQuFk5dnCq"
      },
      "source": [
        "X, Y = df.Text, df.Category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Z06VKY0faW"
      },
      "source": [
        "# lineList = list()\n",
        "# with open('stop_words.txt') as f:\n",
        "#   for line in f:\n",
        "#     lineList.append(line)\n",
        "\n",
        "# lineList = [x.strip() for x in lineList] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "hS__fHJOKG46",
        "outputId": "ad19d1d3-6af1-4042-ac0d-4c1dd7f25324"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\\nهفت اقليم \\nآلودگي هوا پكن را تهديد مي كند \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\nگل و گياه زعفران زينتي \\nنام علمي: Crocus ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\\nيادداشت \\nقانون بودجه و صنايع كوچك \\nدر شمار...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\nدر سالروز ميلاد حضرت مهدي \\nهمايش ادبي دانش ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\\nاز IRA تا فارك \\n بوگوتا، پايتخت پرهرج ومرج ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16673</th>\n",
              "      <td>16673</td>\n",
              "      <td>\\nبراي ارتقاي كيفيت در بخش آموزش عالي \\nوزارت ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16674</th>\n",
              "      <td>16674</td>\n",
              "      <td>\\nديدار رهبر انقلاب اسلامي با \\nگروهي از جوانا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16675</th>\n",
              "      <td>16675</td>\n",
              "      <td>\\nبحران در دولت ائتلافي هند \\nانشعاب در حزب اي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16676</th>\n",
              "      <td>16676</td>\n",
              "      <td>\\nچرا به آثار باستاني خود خنجر؟ مي زنيم! \\nآنط...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16677</th>\n",
              "      <td>16677</td>\n",
              "      <td>\\nيك استاد دانشگاه: قاچاق كالا ناشي ازضعف \\nتو...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16678 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id                                               Text\n",
              "0          0  \\nهفت اقليم \\nآلودگي هوا پكن را تهديد مي كند \\...\n",
              "1          1  \\nگل و گياه زعفران زينتي \\nنام علمي: Crocus ba...\n",
              "2          2  \\nيادداشت \\nقانون بودجه و صنايع كوچك \\nدر شمار...\n",
              "3          3  \\nدر سالروز ميلاد حضرت مهدي \\nهمايش ادبي دانش ...\n",
              "4          4  \\nاز IRA تا فارك \\n بوگوتا، پايتخت پرهرج ومرج ...\n",
              "...      ...                                                ...\n",
              "16673  16673  \\nبراي ارتقاي كيفيت در بخش آموزش عالي \\nوزارت ...\n",
              "16674  16674  \\nديدار رهبر انقلاب اسلامي با \\nگروهي از جوانا...\n",
              "16675  16675  \\nبحران در دولت ائتلافي هند \\nانشعاب در حزب اي...\n",
              "16676  16676  \\nچرا به آثار باستاني خود خنجر؟ مي زنيم! \\nآنط...\n",
              "16677  16677  \\nيك استاد دانشگاه: قاچاق كالا ناشي ازضعف \\nتو...\n",
              "\n",
              "[16678 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0LvaeCwdyYW"
      },
      "source": [
        "# from __future__ import unicode_literals\n",
        "# from hazm import *\n",
        "# import re, string\n",
        "\n",
        "# def remove_stopwords(text):\n",
        "#     text = [word for word in text.split() if word not in lineList]\n",
        "#     return \" \".join(text)\n",
        "\n",
        "# normalizer = Normalizer()\n",
        "\n",
        "\n",
        "# special = re.compile(r'\\W')\n",
        "# single = re.compile(r'\\s+', flags=re.I)\n",
        "\n",
        "# number = re.compile(r\"[-+]?[0-9]+\")\n",
        "# pnumber = re.compile(r\"[-+][\\u06F0-\\u06F90-9]+\")\n",
        "# url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "# html = re.compile(r\"<.*?>\")\n",
        "# emoji_pattern = re.compile(\n",
        "#     \"[\"\n",
        "#     u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "#     u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#     u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#     u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#     u\"\\U00002702-\\U000027B0\"\n",
        "#     u\"\\U000024C2-\\U0001F251\"\n",
        "#     \"]+\",\n",
        "#     flags=re.UNICODE,\n",
        "# )\n",
        "\n",
        "\n",
        "# # df[\"Text\"] = df.Text.map(remove_stopwords)\n",
        "# # df[\"Text\"] = df.Text.map(lambda x: url.sub(r\" \",x))\n",
        "# # df[\"Text\"] = df.Text.map(lambda x: html.sub(r\" \",x))\n",
        "# # df[\"Text\"] = df.Text.map(lambda x: emoji_pattern.sub(r\" \",x))\n",
        "# # df[\"Text\"] = df.Text.map(lambda x: number.sub(r\" \",x))\n",
        "# # df[\"Text\"] = df.Text.map(lambda x: pnumber.sub(r\" \",x))\n",
        "# df[\"Text\"] = df.Text.map(lambda x: x.translate(str.maketrans(\" \", \" \", string.punctuation)))\n",
        "# df[\"Text\"] = df.Text.map(lambda x: special.sub(r\" \",x))\n",
        "# df[\"Text\"] = df.Text.map(lambda x: single.sub(r\" \", x))\n",
        "# # df[\"Text\"] = df.Text.map(lambda x: normalizer.normalize(x))\n",
        "\n",
        "\n",
        "# # df_test[\"Text\"] = df_test.Text.map(remove_stopwords)\n",
        "# # df_test[\"Text\"] = df_test.Text.map(lambda x: url.sub(r\" \",x))\n",
        "# # df_test[\"Text\"] = df_test.Text.map(lambda x: html.sub(r\" \",x))\n",
        "# # df_test[\"Text\"] = df_test.Text.map(lambda x: emoji_pattern.sub(r\" \",x))\n",
        "# # df_test[\"Text\"] = df_test.Text.map(lambda x: number.sub(r\" \",x))\n",
        "# # df_test[\"Text\"] = df_test.Text.map(lambda x: pnumber.sub(r\" \",x))\n",
        "# df_test[\"Text\"] = df_test.Text.map(lambda x: x.translate(str.maketrans(\" \", \" \", string.punctuation)))\n",
        "# df_test[\"Text\"] = df_test.Text.map(lambda x: special.sub(r\" \",x))\n",
        "# df_test[\"Text\"] = df_test.Text.map(lambda x: single.sub(r\" \", x))\n",
        "# # df_test[\"Text\"] = df_test.Text.map(lambda x: normalizer.normalize(x))\n",
        "\n",
        "\n",
        "# # for i in range(df_texts_size):\n",
        "# #   bound = len(df_texts_hazm[i].split(' ')) - 1\n",
        "# #   for char in \"0123456789۰۱۲۳۴۵۶۷۸۹!#$%&().*+,،-/:;<=>?@[\\\\]^_`{|}~\\t\\n\":\n",
        "# #     df_texts_hazm[i] = df_texts_hazm[i].replace(char, ' ', bound)\n",
        "# #   # for word in lineList:\n",
        "# #   #   df_texts_hazm[i] = df_texts_hazm[i].replace(word, ' ', bound)\n",
        "\n",
        "\n",
        "#   # df_texts_hazm[i] = url.sub(r\"\",df_texts_hazm[i])\n",
        "#   # df_texts_hazm[i] = html.sub(r\"\",df_texts_hazm[i])\n",
        "#   # df_texts_hazm[i] = emoji_pattern.sub(r\"\",df_texts_hazm[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdWLyMBChetc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Category, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS07TkCUeXsu"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tfidfconverter = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.8)\n",
        "tfidfconverter = TfidfVectorizer()\n",
        "\n",
        "X = tfidfconverter.fit_transform(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mqYVTeUhk-c",
        "outputId": "6c5fc36d-d81e-49b8-8c05-d4af5738959c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC, LinearSVR\n",
        "from sklearn.svm import SVC, NuSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "\n",
        "\n",
        "# parameters = {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# classifier = GaussianNB()\n",
        "# classifier = GridSearchCV(classifier, parameters, n_jobs=-1)\n",
        "# classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# classifier = RandomForestClassifier(n_estimators=100, criterion='gini',max_depth=10, random_state=0, max_features=None)\n",
        "# classifier_sgc = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=5000)\n",
        "\n",
        "# classifier_linsvc = SVC(kernel='linear',probability=True)\n",
        "# classifier_logreg = LogisticRegression(max_iter=1000)\n",
        "# classifier = KNeighborsClassifier(n_neighbors = 3)\n",
        "# classifier = SVC()\n",
        "# classifier = OneVsRestClassifier(SVC(), n_jobs=-1)\n",
        "# classifier = XGBClassifier()\n",
        "# classifier = LinearSVC(loss=\"hinge\", max_iter=10000, C=3)\n",
        "\n",
        "\n",
        "# classifier = NuSVC(probability=True)\n",
        "# classifier = LinearDiscriminantAnalysis()\n",
        "\n",
        "# classifier_sgc.fit(X, y_train) \n",
        "# classifier_linsvc.fit(X, y_train) \n",
        "# classifier_logreg.fit(X, y_train) \n",
        "\n",
        "# estimators=[('sgc', classifier_sgc), ('svc', classifier_linsvc)]\n",
        "\n",
        "# classifier = VotingClassifier(estimators, voting='hard', n_jobs=-1)\n",
        "\n",
        "# classifier = classifier_sgc \n",
        "\n",
        "# classifier = NBSVM(C=0.001)\n",
        "\n",
        "# classifier.fit(X, y_train) \n",
        "\n",
        "# classifier = PassiveAggressiveClassifier()\n",
        "\n",
        "# pipe = Pipeline(steps = [(\"tfidf_vectorization\", TfidfVectorizer()), (\"classifier\", MultinomialNB())])\n",
        "\n",
        "# search_space = [{\"classifier\": [MultinomialNB()]}, \n",
        "#                 {\"classifier\": [LinearSVC()]},\n",
        "#                 {\"classifier\": [PassiveAggressiveClassifier()]}, \n",
        "#                 {\"classifier\": [LogisticRegression()], \n",
        "#                 \"classifier__solver\": [\"liblinear\"]},\n",
        "#                 {\"classifier\": [KNeighborsClassifier()],\n",
        "#                  \"classifier__n_neighbors\": [6,7,8]}]\n",
        "\n",
        "# classifier = GridSearchCV(estimator=pipe, param_grid=search_space, cv=10, return_train_score=True, n_jobs=-1, refit=\"AUC\")                \n",
        "\n",
        "\n",
        "#Two Best Models:\n",
        "classifier = SGDClassifier(loss=\"modified_huber\", max_iter=100000, alpha=0.00001)\n",
        "# classifier = LinearSVC()\n",
        "\n",
        "classifier.fit(X, y_train) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
              "              max_iter=100000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
              "              validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjH9T-4th1qD"
      },
      "source": [
        "X = tfidfconverter.transform(X_test)\n",
        "y_pred = classifier.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKEJnk0ZmcOE",
        "outputId": "b98aa361-211b-46a1-fde2-e786e7ea70e2"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1568    5    2 ...    2    0    0]\n",
            " [  15   20    0 ...    0    0    0]\n",
            " [  41    1   27 ...    0    0    1]\n",
            " ...\n",
            " [   2    0    0 ... 1437    0    0]\n",
            " [   0    0    0 ...   16    4    0]\n",
            " [   4    0    0 ...    1    0   15]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                                      precision    recall  f1-score   support\n",
            "\n",
            "                                                             Economy       0.79      0.89      0.83      1767\n",
            "                                                 Economy.Agriculture       0.74      0.47      0.57        43\n",
            "                                             Economy.Bank and Bourse       0.84      0.32      0.46        85\n",
            "                                                    Economy.Commerce       0.67      0.06      0.11        33\n",
            "                                   Economy.Dwelling and Construction       0.00      0.00      0.00         4\n",
            "                                                    Economy.Industry       0.31      0.14      0.19        86\n",
            "                                                         Economy.Oil       0.84      0.69      0.76       108\n",
            "                                                  Literature and Art       0.63      0.65      0.64       450\n",
            "                                              Literature and Art.Art       0.73      0.68      0.70       105\n",
            "                                       Literature and Art.Art.Cinema       1.00      0.06      0.11        17\n",
            "                                        Literature and Art.Art.Music       0.75      0.21      0.33        14\n",
            "                                      Literature and Art.Art.Theater       0.00      0.00      0.00         4\n",
            "                                       Literature and Art.Literature       0.00      0.00      0.00         3\n",
            "                                                       Miscellaneous       0.80      0.79      0.80      3326\n",
            "                                            Miscellaneous.Happenings       0.79      0.73      0.76       325\n",
            "                                      Miscellaneous.Islamic Councils       0.66      0.60      0.63        65\n",
            "                                               Miscellaneous.Picture       0.98      0.98      0.98        49\n",
            "                                    Miscellaneous.Picture.Caricature       1.00      1.00      1.00        17\n",
            "                                                 Miscellaneous.Urban       0.75      0.74      0.75      1693\n",
            "                                            Miscellaneous.World News       0.90      0.94      0.92      1381\n",
            "                                                 Natural Environment       0.77      0.28      0.41        82\n",
            "                                                            Politics       0.82      0.82      0.82      1736\n",
            "                                              Politics.Iran Politics       0.71      0.47      0.57        74\n",
            "                                                 Science and Culture       0.73      0.78      0.75      1043\n",
            "                                         Science and Culture.Science       0.78      0.69      0.73        90\n",
            "                                    Science and Culture.Science.Book       0.83      0.22      0.34        23\n",
            "Science and Culture.Science.Information and Communication Technology       0.76      0.70      0.73        23\n",
            "                     Science and Culture.Science.Medicine and Remedy       0.00      0.00      0.00         4\n",
            "                                                              Social       0.68      0.73      0.70       762\n",
            "                                                     Social.Religion       0.66      0.56      0.61        48\n",
            "                                                        Social.Women       0.50      0.27      0.35        30\n",
            "                                                               Sport       0.97      0.98      0.97      1461\n",
            "                                                     Sport.World Cup       1.00      0.18      0.31        22\n",
            "                                                             Tourism       0.83      0.41      0.55        37\n",
            "\n",
            "                                                            accuracy                           0.80     15010\n",
            "                                                           macro avg       0.68      0.50      0.54     15010\n",
            "                                                        weighted avg       0.80      0.80      0.80     15010\n",
            "\n",
            "0.8016655562958028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpPw85E2oFH3"
      },
      "source": [
        "X = tfidfconverter.transform(df_test.Text)\n",
        "Y_final = classifier.predict(X)\n",
        "df_test_res = df_test.copy()\n",
        "df_test_res.Text = Y_final\n",
        "df_test_res = df_test_res.rename(columns = {\"Text\":\"Category\"}) \n",
        "df_test_res.to_csv(\"Final.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMEIipogXmpz"
      },
      "source": [
        "with open('text_classifier', 'wb') as picklefile:\n",
        "    pickle.dump(classifier,picklefile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUIQD5xhYXGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2f7339-fac1-4b0a-c9fa-d4225d0c0f06"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  Final.csv  sample_data  text_classifier\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}